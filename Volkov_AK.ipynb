{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrmL8zM/he+ZRlV5fwIaBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7vckingrck/VKR/blob/test/Volkov_AK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtCEAqOdn4lM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sqlite3\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "def generate_network_data(num_records=100000, start_date=\"2023-01-01\", end_date=\"2025-03-28\"):\n",
        "    services = [\"HTTP\", \"HTTPS\", \"SSH\", \"FTP\", \"DNS\", \"SMTP\", \"ICMP\"]\n",
        "    mac_prefixes = [\"00:1A:79\", \"00:0D:3A\", \"00:24:BE\"]\n",
        "    ip_ranges = [\"192.168.1.\", \"10.0.0.\", \"172.16.0.\"]\n",
        "    device_types = [\"Smartphone\", \"Laptop\", \"IoT\", \"Tablet\"]\n",
        "\n",
        "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        \"Timestamp\": [start + timedelta(seconds=random.randint(0, int((end - start).total_seconds())))\n",
        "                     for _ in range(num_records)],\n",
        "        \"WAN IP\": [f\"{random.choice(ip_ranges)}{random.randint(1, 254)}\" for _ in range(num_records)],\n",
        "        \"MAC Address\": [f\"{random.choice(mac_prefixes)}:{random.randint(0, 255):02X}:\"\n",
        "                       f\"{random.randint(0, 255):02X}:{random.randint(0, 255):02X}\"\n",
        "                       for _ in range(num_records)],\n",
        "        \"Service\": np.random.choice(services, num_records),\n",
        "        \"Time Spent (s)\": np.random.randint(1, 3600, num_records),\n",
        "        \"Traffic Volume (MB)\": np.round(np.random.uniform(0.1, 500, num_records), 2),\n",
        "        \"Device_Type\": np.random.choice(device_types, num_records),\n",
        "        \"Signal_Strength\": np.random.randint(-90, -30, num_records)\n",
        "    })\n",
        "\n",
        "    anomaly_mask = np.random.rand(num_records) < 0.05\n",
        "    data.loc[anomaly_mask, 'Time Spent (s)'] = np.random.randint(3600, 86400, sum(anomaly_mask))\n",
        "    data.loc[anomaly_mask, 'Traffic Volume (MB)'] = np.random.uniform(1000, 5000, sum(anomaly_mask))\n",
        "    data.loc[anomaly_mask, 'Service'] = np.random.choice([\"SSH\", \"FTP\", \"ICMP\"], sum(anomaly_mask))\n",
        "\n",
        "    return data\n",
        "\n",
        "def load_and_prepare_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df[(df['Time Spent (s)'] > 0) & (df['Traffic Volume (MB)'] >= 0)]\n",
        "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "    df['Hour_sin'] = np.sin(2*np.pi*df['Timestamp'].dt.hour/24)\n",
        "    df['Hour_cos'] = np.cos(2*np.pi*df['Timestamp'].dt.hour/24)\n",
        "    return df\n",
        "\n",
        "def generate_features(df):\n",
        "    df['Connections_per_MAC_1h'] = df.groupby(['MAC Address', pd.Grouper(key='Timestamp', freq='1H')])['MAC Address'].transform('count')\n",
        "    df['Traffic_per_connection'] = df['Traffic Volume (MB)'] / (df['Time Spent (s)'] + 1e-6)\n",
        "    df['Traffic_std'] = df.groupby('MAC Address')['Traffic Volume (MB)'].transform('std')\n",
        "    df['IP_rotation_rate'] = df.groupby('MAC Address')['WAN IP'].transform('nunique') / df.groupby('MAC Address').size()\n",
        "    return df.fillna(0)\n",
        "\n",
        "class AnomalyDetector:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'IsolationForest': make_pipeline(\n",
        "                RobustScaler(),\n",
        "                IsolationForest(n_estimators=500, contamination='auto', random_state=42)\n",
        "            ),\n",
        "            'OneClassSVM': make_pipeline(\n",
        "                RobustScaler(),\n",
        "                OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')\n",
        "            ),\n",
        "            'LOF': make_pipeline(\n",
        "                RobustScaler(),\n",
        "                LocalOutlierFactor(n_neighbors=50, contamination=0.05, novelty=True)\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def fit(self, X):\n",
        "        for model in self.models.values():\n",
        "            model.fit(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models.values()])\n",
        "        return (np.mean(predictions, axis=0) < 0).astype(int)\n",
        "\n",
        "def build_network_graph(df):\n",
        "    G = nx.Graph()\n",
        "    for mac in df['MAC Address'].unique():\n",
        "        G.add_node(mac, type='device')\n",
        "\n",
        "    ip_groups = df.groupby('WAN IP')['MAC Address'].unique()\n",
        "    for ip, macs in ip_groups.items():\n",
        "        if len(macs) > 1:\n",
        "            for i in range(len(macs)):\n",
        "                for j in range(i+1, len(macs)):\n",
        "                    if G.has_edge(macs[i], macs[j]):\n",
        "                        G[macs[i]][macs[j]]['weight'] += 1\n",
        "                    else:\n",
        "                        G.add_edge(macs[i], macs[j], weight=1)\n",
        "    return G\n",
        "\n",
        "def plot_3d_anomalies(df, anomalies):\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    normal = df[~anomalies]\n",
        "    anomaly = df[anomalies]\n",
        "\n",
        "    ax.scatter(normal['Time Spent (s)'], normal['Traffic Volume (MB)'], normal['Connections_per_MAC_1h'],\n",
        "               c='blue', alpha=0.3, label='Normal')\n",
        "    ax.scatter(anomaly['Time Spent (s)'], anomaly['Traffic Volume (MB)'], anomaly['Connections_per_MAC_1h'],\n",
        "               c='red', alpha=0.7, label='Anomaly')\n",
        "\n",
        "    ax.set_xlabel('Time Spent (s)')\n",
        "    ax.set_ylabel('Traffic (MB)')\n",
        "    ax.set_zlabel('Connections/Hour')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "class StreamingAnomalyDetector:\n",
        "    def __init__(self, window_size=1000):\n",
        "        self.window = []\n",
        "        self.window_size = window_size\n",
        "        self.model = IsolationForest(n_estimators=100)\n",
        "        self.scaler = RobustScaler()\n",
        "\n",
        "    def process_record(self, record):\n",
        "        self.window.append(record)\n",
        "        if len(self.window) >= self.window_size:\n",
        "            self._train_model()\n",
        "            self.window = []\n",
        "        return self._predict(record)\n",
        "\n",
        "    def _train_model(self):\n",
        "        X = pd.DataFrame(self.window)[features]\n",
        "        self.model.fit(self.scaler.fit_transform(X))\n",
        "\n",
        "    def _predict(self, record):\n",
        "        x = self.scaler.transform([record[features]])\n",
        "        return self.model.predict(x)[0]\n",
        "\n",
        "class DatabaseManager:\n",
        "    def __init__(self, db_path=\"network_data.db\"):\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self._init_db()\n",
        "\n",
        "    def _init_db(self):\n",
        "        self.conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS connections (\n",
        "            timestamp TEXT, mac TEXT, ip TEXT, service TEXT,\n",
        "            duration INTEGER, traffic REAL, device_type TEXT,\n",
        "            signal_strength INTEGER, is_anomaly INTEGER DEFAULT 0\n",
        "        )\"\"\")\n",
        "        self.conn.commit()\n",
        "\n",
        "    def save_record(self, record):\n",
        "        self.conn.execute(\"\"\"\n",
        "        INSERT INTO connections VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", record)\n",
        "        self.conn.commit()\n",
        "\n",
        "def main(mode='analyze'):\n",
        "    if not os.path.exists(\"data.csv\"):\n",
        "        df = generate_network_data()\n",
        "        df.to_csv(\"data.csv\", index=False)\n",
        "\n",
        "    df = load_and_prepare_data(\"data.csv\")\n",
        "    df = generate_features(df)\n",
        "\n",
        "    features = [\n",
        "        'Time Spent (s)', 'Traffic Volume (MB)', 'Connections_per_MAC_1h',\n",
        "        'Traffic_per_connection', 'Traffic_std', 'IP_rotation_rate',\n",
        "        'Hour_sin', 'Hour_cos'\n",
        "    ]\n",
        "    X = df[features]\n",
        "    y = ((df['Traffic Volume (MB)'] > 1000) | (df['Connections_per_MAC_1h'] > 100)).astype(int)\n",
        "\n",
        "    if mode == 'analyze':\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        detector = AnomalyDetector()\n",
        "        detector.fit(X_train)\n",
        "        y_pred = detector.predict(X_test)\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "        plot_3d_anomalies(df.iloc[X_test.index], y_test)\n",
        "\n",
        "        G = build_network_graph(df)\n",
        "        print(f\"\\nNetwork Graph: {len(G.nodes())} nodes, {len(G.edges())} edges\")\n",
        "\n",
        "    elif mode == 'stream':\n",
        "        stream_processor = StreamingAnomalyDetector()\n",
        "        db = DatabaseManager()\n",
        "\n",
        "        for _, row in df.sample(1000).iterrows():\n",
        "            record = row[features].tolist() + [row['Device_Type'], row['Signal_Strength']]\n",
        "            is_anomaly = stream_processor.process_record(record)\n",
        "\n",
        "            db.save_record((\n",
        "                str(row['Timestamp']), row['MAC Address'], row['WAN IP'],\n",
        "                row['Service'], row['Time Spent (s)'], row['Traffic Volume (MB)'],\n",
        "                row['Device_Type'], row['Signal_Strength'], int(is_anomaly)\n",
        "            ))\n",
        "\n",
        "            if is_anomaly == -1:\n",
        "                print(f\"Anomaly detected: {row['MAC Address']} at {row['Timestamp']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--mode', choices=['analyze', 'stream'], default='analyze')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(mode=args.mode)"
      ]
    }
  ]
}